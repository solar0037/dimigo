빅 데이터(영어: big data)란 기존 데이터베이스 관리도구의 능력을 넘어서는 대량(수십 테라바이트)의 정형 또는 심지어 데이터베이스 형태가 아닌 비정형의 데이터 집합조차 포함한[1] 데이터로부터 가치를 추출하고 결과를 분석하는 기술[2]이다. 즉, 데이터 베이스 등 기존의 데이터 처리 응용 소프트웨어(data-processing application software)로는 수집 · 저장 · 분석 · 처리하기 어려울 정도로 방대한 양의 데이터를 의미한다.


위키백과의 편집 현황의 시각화 자료(IBM 작성). 수 테라바이트의 용량을 지닌 위키백과의 텍스트 및 이미지 자료는 빅 데이터의 고전적 사례에 속한다.

전세계 저장 매체 용량의 증가 및 디지털화.(출처: 워싱턴 포스트)
다양한 종류의 대규모 데이터에 대한 생성, 수집, 분석, 표현을 그 특징으로 하는 빅 데이터 기술의 발전은 다변화된 현대 사회를 더욱 정확하게 예측하여 효율적으로 작동케한다. 개인화된 현대 사회 구성원마다 맞춤형 정보를 제공, 관리, 분석이 가능해 과거에는 불가능했던 기술을 실현시키기도 한다.

이와 같이 빅 데이터는 정치, 사회, 경제, 문화, 과학 기술 등 전 영역에 걸쳐서 사회와 인류에게 가치 있는 정보를 제공할 수 있는 가능성을 제시하며 그 중요성이 부각되고 있다.

하지만 빅데이터의 문제점은 바로 사생활 침해와 보안 측면에 자리하고 있다. 빅데이터는 수많은 개인들의 수많은 정보의 집합이다. 그렇기에 빅데이터를 수집, 분석할 때에 개인들의 사적인 정보까지 수집하여 관리하는 빅브라더의 모습이 될 수도 있는 것이다. 그리고 그렇게 모은 데이터가 보안 문제로 유출된다면, 이 역시 거의 모든 사람들의 정보가 유출되는 것이기에 큰 문제가 될 수 있다.

세계 경제 포럼은 2012년 떠오르는 10대 기술 중 그 첫 번째를 빅 데이터 기술로 선정[3] 했으며 대한민국 지식경제부 R&D 전략기획단은 IT 10대 핵심기술 가운데 하나로 빅 데이터를 선정[4] 하기도 했다.

정의
빅 데이터는 통상적으로 사용되는 데이터 수집, 관리 및 처리 소프트웨어의 수용 한계를 넘어서는 크기의 데이터를 말한다.[5] 빅 데이터의 사이즈는 단일 데이터 집합의 크기가 수십 테라바이트에서 수 페타바이트에 이르며, 그 크기가 끊임없이 변화하는 것이 특징이다. 빅데이터라는 용어는 1990년대부터 사용되어 왔으며, 존 매쉬가 이 용어를 대중화하였다[6][7].

특징[8]과 의미
빅 데이터의 공통적 특징은 3V로 설명할 수 있다. 속도는 대용량의 데이터를 빠르게 처리하고 분석할 수 있는 속성이다. 융복합 환경에서 디지털 데이터는 매우 빠른 속도로 생산되므로 이를 실시간으로 저장, 유통, 수집, 분석처리가 가능한 성능을 의미한다. 다양성(Variety)은 다양한 종류의 데이터를 의미하며 정형화의 종류에 따라 정형, 반정형, 비정형 데이터로 분류할 수 있다. 빅데이터의 특징은 3V로 요약하는 것이 일반적이다. 즉 데이터의 양(Volume), 데이터 생성 속도(Velocity), 형태의 다양성(Variety)을 의미한다. 최근에는 가치(Value)나 복잡성(Complexity)을 덧붙이기도 한다. 이처럼 다양하고 방대한 규모의 데이터는 미래 경쟁력의 우위를 좌우하는 중요한 자원으로 활용될 수 있다는 점에서 주목받고 있다. 대규모 데이터를 분석해서 의미있는 정보를 찾아내는 시도는 예전에도 존재했다. 그러나 현재의 빅데이터 환경은 과거와 비교해 데이터의 양은 물론 질과 다양성 측면에서 패러다임의 전환을 의미한다.이런 관점에서 빅데이터는 산업혁명 시기의 석탄처럼 IT와 스마트혁명 시기에 혁신과 경쟁력 강화, 생산성 향상을 위한 중요한 원천으로 간주되고 있다. 데이터 속도(Velocity)은 다양한 형태의 데이터를 포함하는 것을 뜻한다. 정형 데이터 뿐만 아니라 사진, 오디오, 비디오, 소셜 미디어 데이터, 로그 파일 등과 같은 비정형 데이터도 포함된다.

빅 데이터의 새로운 V[9]
빅 데이터의 새로운 V - 정확성(Veracity) : 빅 데이터 시대에는 방대한 데이터의 양을 분석하여 일정한 패턴을 추출할 수 있다. 하지만 정보의 양이 많아지는 만큼 데이터의 신뢰성이 떨어지기 쉽다. 따라서 빅데이터를 분석하는데 있어 기업이나 기관에 수집한 데이터가 정확한 것인지, 분석할 만한 가치가 있는지 등을 살펴야 하는 필요성이 대두되었고, 이러한 측면에서 새로운 속성인 정확성(Veracity)이 제시되고 있다.
빅 데이터의 새로운 V - 가변성(Variability) : 최근 소셜미디어의 확산으로 자신의 의견을 웹사이트를 통해 자유롭게 게시하는 것이 쉬워졌지만 실제로 자신의 의도와는 달리 자신의 생각을 글로 표현하게 되면 맥락에 따라 자신의 의도가 다른 사람에게 오해를 불러일으킬 수 있다. 이처럼 데이터가 맥락에 따라 의미가 달라진다고 하여 빅 데이터의 새로운 속성으로 가변성(Variability)이 제시되고 있다.
빅 데이터의 새로운 V - 시각화(Visualization) : 빅 데이터는 정형 및 비정형 데이터를 수집하여 복잡한 분석을 실행한 후 용도에 맞게 정보를 가공하는 과정을 거친다. 이때 중요한 것은 정보의 사용대상자의 이해정도이다. 그렇지 않으면 정보의 가공을 위해 소모된 시간적, 경제적 비용이 무용지물이 될 수 있기 때문이다.
메타그룹(현재 가트너)의 애널리스트 더그 레이니(Doug Laney)는 2001년 그의 연구 보고서[10]와 관련 강의에서 데이터의 급성장에 따른 이슈와 기회를 데이터의 양(volume), 데이터 입출력의 속도(velocity), 데이터 종류의 다양성(variety)이라는 세 개의 차원으로 정의하였다. 이 “3V” 모델은 이후 가장 널리 사용되는 빅 데이터의 정의가 되었다.[11] 2012년 가트너는 기존의 정의를 다음과 같이 개정하였다: “빅 데이터는 큰 용량, 빠른 속도, 그리고(또는) 높은 다양성을 갖는 정보 자산으로서 이를 통해 의사 결정 및 통찰 발견, 프로세스 최적화를 향상시키기 위해서는 새로운 형태의 처리 방식이 필요하다.”[12] 이에 더해, IBM은 정확성(Veracity)이라는 요소를 더해 4V를 정의하였고,[13] 브라이언 홉킨스(Brian Hopkins) 등은 가변성(Variability)을 추가하여 4V를 정의하였다.[14]

가트너의 3V 정의가 여전히 널리 사용되고 있는 가운데, 데이터와 그것의 사용 방법에 있어서 빅 데이터와 경영정보학의 차이가 점차 더 뚜렷하게 구분되고 있다.

경영정보학은 대상을 측정하고 경향을 예측하는 등의 일을 하기 위해 고밀도의 데이터로 구성된 기술적 통계를 활용한다.
빅 데이터는 큰 데이터 집합으로부터 일정한 법칙을 추론하여 결과 및 행동을 예측하기 위해 통계적 추론과 비선형 시스템 식별(nonlinear system identification)[15]의 일부 개념을 활용한다.[15][16]
분석 기법
상기 특징을 가진 빅 데이터의 분석, 활용을 위한 빅 데이터 처리 기법은 크게 분석 기술, 표현 기술로 나뉜다.

분석 기술
빅데이터를 다루는 처리 프로세스로서 병렬 처리의 핵심은 분할 점령(Divide and Conquer)이다. 즉 데이터를 독립된 형태로 나누고 이를 병렬적으로 처리하는 것을 말한다. 빅데이터의 데이터 처리란 이렇게 문제를 여러 개의 작은 연산으로 나누고 이를 취합하여 하나의 결과로 만드는 것을 뜻한다. 대용량의 데이터를 처리하는 기술 중 가장 널리 알려진 것은 아파치 하둡()과 같은 Map-Reduce 방식의 분산 데이터 처리 프레임워크이다. 대부분의 빅 데이터 분석 기술과 방법들은 기존 통계학과 전산학에서 사용되던 데이터마이닝, 기계 학습, 자연 언어 처리, 패턴 인식 등이 해당한다.[17] 소셜 미디어등 비정형 데이터의 증가로 인해 분석기법 중에서 텍스트 마이닝, 오피니언 마이닝, 소셜네트워크 분석, 군집분석 등이 주목받고 있다.[18]

아파치 하둡(Apache Hadoop) : 대량의 자료를 처리할 수 있는 큰 컴퓨터 클러스터에서 동작하는 분산 응용 프로그램을 지원하는 프리웨어 자바 소프트웨어 프레임워크
텍스트 마이닝: 비/반정형 텍스트 데이터에서 자연 언어 처리 기술에 기반을 두어 유용한 정보를 추출, 가공
오피니언 마이닝: 소셜미디어 등의 정형/비정형 텍스트의 긍정, 부정, 중립의 선호도를 판별
소셜 네트워크 분석: 소셜 네트워크의 연결 구조 및 강도 등을 바탕으로 사용자의 명성 및 영향력을 측정
군집 분석: 비슷한 특성을 가진 개체를 합쳐가면서 최종적으로 유사 특성의 군집을 발굴
대규모의 정형/비정형 데이터를 처리하는 데 있어 가장 기본적인 분석 인프라로 하둡이 있으며 데이터를 유연하고 더욱 빠르게 처리하기 위해 NoSQL 기술이 활용되기도 한다.[18]

표현 기술
빅 데이터 분석 기술을 통해 분석된 데이터의 의미와 가치를 시각적으로 표현하기 위한 기술로 대표적인 것으로는 R (프로그래밍 언어)이 있다.[18]

빅데이터 플랫폼[19]
빅데이터 플랫폼은 빅데이터 기술의 집합체이자 기술을 잘 사용할 수 있도록 준비된 환경이다. 기업들은 빅데이터 플랫폼을 사용하여 빅데이터를 수집, 저장, 처리 및 관리 할 수 있다. 빅데이터 플랫폼은 빅데이터를 분석하거나 활용하는 데 필요한 필수 인프라(Infrastructure)인 셈이다. 빅데이터 플랫폼은 빅데이터라는 원석을 발굴하고, 보관, 가공하는 일련의 과정을 이음새 없이 통합적으로 제공해야 한다. 이러한 안정적 기반 위에서 전처리된 데이터를 분석하고 이를 다시 각종 업무에 맞게 가공하여 활용한다면 사용자가 원하는 가치를 정확하게 얻을 수 있을 것이다.

활용사례 및 의의
정치
2008년 미국 대통령 선거
2008년 미국 대통령 선거에서 버락 오바마 미국 대통령 후보는 다양한 형태의 유권자 데이터베이스를 확보하여 이를 분석, 활용한 '유권자 맞춤형 선거 전략'을 전개했다. 당시 오바마 캠프는 인종, 종교, 나이, 가구형태, 소비수준과 같은 기본 인적 사항으로 유권자를 분류하는 것을 넘어서서 과거 투표 여부, 구독하는 잡지, 마시는 음료 등 유권자 성향까지 전화나 개별 방문을 또는 소셜 미디어를 통해 유권자 정보를 수집하였다. 수집된 데이터는 오바마 캠프 본부로 전송되어 유권자 데이터베이스를 온라인으로 통합관리하는 ‘보트빌더(VoteBuilder.com)’시스템의 도움으로 유권자 성향 분석, 미결정 유권자 선별 , 유권자에 대한 예측을 해나갔다. 이를 바탕으로‘유권자 지도’를 작성한 뒤 ‘유권자 맞춤형 선거 전략’을 전개하는 등 오바마 캠프는 비용 대비 효과적인 선거를 치를 수 있었다.

대한민국 제19대 총선
중앙선거관리위원회는 대한민국 제19대 총선부터 소셜 네트워크 등 인터넷 상의 선거 운동을 상시 허용하였다.[20] 이에 소셜 미디어 상에서 선거 관련 데이터는 증폭되었으며, 2010년 대한민국 제5회 지방 선거 및 2011년 대한민국 재보궐선거에서 소셜 네트워크 서비스의 중요성을 확인한 정당들 또한 SNS 역량 지수를 공천 심사에 반영하는 등[21] 소셜 네트워크 활용에 주목했다. 이 가운데 여론 조사 기관들은 기존 여론조사 방식으로 예측한 2010년 제5회 지방 선거 및 2011년 재보궐선거의 여론조사 결과와 실제 투표 결과와의 큰 차이를 보완하고자 빅 데이터 기술을 활용한 SNS 여론 분석을 시행했다. 그러나 SNS 이용자의 대다수가 수도권 20~30대에 쏠려 있기에[22], 빅 데이터를 이용한 대한민국 제19대 총선에 대한 SNS 분석은 수도권으로 한정되어 일치하는 한계를 드러내기도 하였다.

경제 및 경영
아마존닷컴의 추천 상품 표시 / 구글 및 페이스북의 맞춤형 광고
아마존닷컴은 모든 고객들의 구매 내역을 데이터베이스에 기록하고, 이 기록을 분석해 소비자의 소비 취향과 관심사를 파악한다.[23] 이런 빅 데이터의 활용을 통해 아마존은 고객별로 '추천 상품(레코멘데이션)'을 표시한다. 고객 한사람 한사람의 취미나 독서 경향을 찾아 그와 일치한다고 생각되는 상품을 메일, 홈 페이지상에서 중점적으로 고객 한사람 한사람에게 자동적으로 제시하는 것이다.[24] 아마존닷컴의 추천 상품 표시와 같은 방식으로 구글 및 페이스북도 이용자의 검색 조건, 나아가 사진과 동영상 같은 비정형 데이터 사용을 즉각 처리하여 이용자에게 맞춤형 광고를 제공하는 등 빅데이터의 활용을 증대시키고 있다.

사회
코로나19와 이민자, 유학생 간 상관관계
코로나19 확진자의 국가별 통계와 중국인 유학생 이동 통계, 중국인 이민자 수 통계를 이용한 상관성 분석을 통해 코로나19의 세계적 확산 양상이 중국 이민자 및 유학생 진출자 수와 비교적 강한 상관관계를 지닌다는 국내 연구팀의 분석 결과가 나왔다.[25] 중국인이 감염증 발생 및 확산의 원인이라는 결론은 위험하며, 정보 분석을 통해 감염병 확산을 예측하고 효과적으로 대처할 수 있다는 사실을 알리기 위한 분석의 한 사례이다.

문화
MLB (메이저 리그 베이스볼)의 머니볼 이론 및 데이터 야구
머니볼 이론이란 경기 데이터를 철저하게 분석해 오직 데이터를 기반으로 적재적소에 선수들을 배치해 승률을 높인다는 게임 이론이다.[26] 이는 미국 메이저 리그 베이스볼 오클랜드 어슬레틱스의 구단장 빌리 빈이 리그 전체 25위에 해당하는 낮은 구단 지원금 속에서도 최소비용으로 최대효과를 거둔 상황에서 유래되었다. 빌리 빈은 최하위에 그치던 팀을 4년 연속 포스트시즌에 진출시키고 메이저 리그 최초로 20연승이라는 신기록을 세우도록 탈바꿈 시켰다. 2003년, 미국 월스트리트 저널은 미국 경제에 큰 영향을 끼치는 파워 엘리트 30인에 워렌 버핏, 앨런 그린스펀과 함께 빌리 빈을 선정[27] 하는 등 머니볼 이론은 경영, 금융 분야에서도 주목받았다. 최근 들어서 과학기술 및 카메라 기술의 발달로 더욱 정교한 데이터의 수집이 가능해졌으며 투구의 궤적 및 투수의 그립, 타구 방향, 야수의 움직임까지 잡아낼 수 있게 되었다. 이처럼 기존의 정형 데이터뿐만 아닌 비정형 데이터의 수집과 분석, 활용을 통해 최근 야구경기에서 빅 데이터의 중요성은 더욱 커지고 있다. 선수의 인기만을 쫓는 것이 아니라 팀별 승률이나 선수의 성적을 나타내는 수치와 야구를 관전한다면 그 재미는 배가된다. '출루율'은 타율로 인정되지 않는 볼넷을 포함하여 타자가 성공적으로 베이스를 밟은 횟수의 비율, '장타율'은 타수마다 밟은 총 베이스를 계산해서 타격력이 얼마나 강한지를 나타내는 비율이다.

출루율과 장타율 못지 않게 '타수'는 한두 경기에서 낸 성적이 아닌, 수천 번의 타석에 들어 좋은 성적을 만들어낸 선수를 선별하기 위한 기초 통계자료이다. 이처럼 한 선수의 타율에서 팀의 역대 시리즈 전적까지 모든 것을 숫자로 표현할 수 있다고 해서 야구를 '통계의 스포츠'라고 부르기도 한다. 야구뿐만 아니라 생활 곳곳에서 활용되는 통계는 복잡한 상황과 설명을 간단한 숫자로 바꿔주는 매우 강력한 도구이다.[28]

'프로파일링'과 '빅데이터' 기법을 활용한 프로그램 MBC <프로파일링>
방송에는 19세 소년의 살인 심리를 파헤친 '용인살인사건의 재구성', 강남 3구 초등학교 85곳의 학업성취도평가 성적과 주변 아파트 매매가의 상관관계를 빅데이터(디지털 환경에서 발생한 방대한 규모의 데이터)를 통해 분석한 '강남, 부자일수록 공부를 잘할까'[29]

2014년 FIFA 월드컵 독일 우승과 '빅데이터'
브라질에서 개최된 2014년 FIFA 월드컵에서 독일은 준결승에서 개최국인 브라질을 7:1로 꺾고, 결승에서 아르헨티나와 연장전까지 가는 접전 끝에 1:0으로 승리를 거두었다. 무패행진으로 우승을 차지한 독일 국가대표팀의 우승의 배경에는 '빅데이터'가 있었다.

독일 국가대표팀은 SAP와 협업하여 훈련과 실전 경기에 'SAP 매치 인사이트'를 도입했다. SAP 매치 인사이트란 선수들에게 부착된 센서를 통해 운동량, 순간속도, 심박수, 슈팅동작 등 방대한 비정형 데이터를 수집, 분석한 결과를 감독과 코치의 태블릿PC로 전송하여 그들이 데이터를 기반으로 전술을 짜도록 도와주는 솔루션이다. 기존에 감독의 경험이나 주관적 판단으로 결정되는 전략과는 달리, SAP 매치 인사이트를 통해 이루어지는 분석은 선수들에 대한 분석 뿐만 아니라 상대팀 전력, 강점, 약점 등 종합적인 분석을 통해 좀 더 과학적인 전략을 수립할 수 있다. 정보 수집에 쓰이는 센서 1개가 1분에 만들어내는 데이터는 총 12000여개로 독일 국가대표팀은 선수당 4개(골키퍼는 양 손목을 포함해 6개)의 센서를 부착했고, 90분 경기동안 한 선수당 약 432만개, 팀 전체로 약 4968만개의 데이터를 수집했다고 한다.월드컵8강 獨 전차군단 비밀병기는 '빅데이터'

여론의 장으로서의 유튜브
유튜브가 폭발적 성장한 것은 2011년에 트위터와 페이스북 등 소셜미디어와 유사하게 개편되면서 구독하기·댓글달기·추천동영상 등 사람들이 쉽게 관계를 맺고 적극적 참여가 가능하도록 만들었다.[30] 이는 오락적 콘텐츠 소비 차원을 넘어 새로운 여론의 장을 만들어가고 있다. 다음 아고라로 시작된 인터넷 여론의 창은 트위터, 페이스북을 거쳐 유튜브로 이동하고 있다. 빅 데이터는 언급 빈도의 단순 집계와 통계적 분포는 물론 해당 단어들이 사용된 정서적 맥락과 제3의 단어와 가지는 관계성을 고려한 분석이 가능하다.[31]

과학기술 및 활용
통계학
데이터 마이닝이란 기존 데이터베이스 관리도구의 데이터 수집, 저장, 관리, 분석의 역량을 넘어서는 대량의 정형 또는 비정형 데이터 집합 및 이러한 데이터로부터 가치를 추출하고 결과를 분석하는 기술로, 수집되는 ‘빅 데이터’를 보완하고 마케팅, 시청률조사, 경영 등으로부터 체계화돼 분류, 예측, 연관분석 등의 데이터 마이닝을 거쳐 통계학적으로 결과를 도출해 내고 있다.[32][33]

대한민국에서는 2000년부터 정보통신부의 산하단체로 사단법인 한국BI데이터마이닝학회가 설립되어 데이터 마이닝에 관한 학술과 기술을 발전, 보급, 응용하고 있다. 또한 국내ㆍ외 통계분야에서 서서히 빅 데이터 활용에 대한 관심과 필요성이 커지고 있는 가운데 국가통계 업무를 계획하고 방대한 통계자료를 처리하는 국가기관인 통계청이 빅 데이터를 연구하고 활용방안을 모색하기 위한 '빅 데이터 연구회'를 발족하였다.[34] 하지만 업계에 따르면, 미국과 영국, 일본 등 선진국들은 이미 빅 데이터를 다각적으로 분석해 조직의 전략방향을 제시하는 데이터과학자 양성에 사활을 걸고 있다. 그러나 한국은 정부와 일부 기업이 데이터과학자 양성을 위한 프로그램을 진행 중에 있어 아직 걸음마 단계인 것으로 알려져 있다.[35]

생물정보학
최근 생물학에서 DNA, RNA, 단백질 서열 및 유전자들의 발현과 조절에 대한 데이터의 양이 급격히 증가했고 이에 따라 이 빅데이터를 활용한 생명의 이해에 관한 논의가 진행되고 있다.

빅데이터 시대의 초고속 SSD[36]
SSD는 대용량 데이터를 처리하고 관리 하는 데이터센터, 클라우드 등에서 많은 인기를 끌고 있으며, 실제로 국내·외의 하드웨어 업체들은 소비자용 SSD를 넘어 데이터센터 기반의 기업들을 대상으로 한 기업용 SSD를 출시하여 시장의 입지를 다져가며 넓혀가 있는 중이다. 시장조사기관 IHS 마킷(IHS Markit) 에 따르면 기업용 SSD 시장은 올해 142억달러로 꾸준히 성장하여 2021년에는 176억달러로 늘어나며 연평균 7.0% 성장을 이끌어낼 전망이다.

보건의료
국민건강보험공단은 가입자의 자격·보험료, 진료·투약내용, 건강검진 결과 및 생활습관 정보 등 2조1천억건, 92테라바이트의 빅데이터를 보유하고 있고, 한편, 건강보험심사평가원은 진료내역, 투약내용(의약품 안심서비스), 의약품 유통 등의 2조2천억건, 89테라바이트의 빅데이터를 보유하고 있으며, 경제협력개발기구(OECD)는 한국의 건강보험 빅데이터 순위가 2위라고 발표했었다. 건보공단과 심평원은 빅데이터를 민간에 널리 알리고 더 많이 개방하고 있다. (연합뉴스 2016.6.14 인터넷뉴스 참조)

빅 데이터를 활용하면 미국 의료부문은 연간 3,300 억 달러(미 정부 의료 예산의 약 8%에 해당하는 규모)의 직간접적인 비용 절감 효과를 보일 것으로 전망된다.[37] 특히 임상분야에서는 의료기관 별 진료방법, 효능, 비용 데이터를 분석하여 보다 효과적인 진료방법을 파악하고 환자 데이터를 온라인 플랫폼화하여 의료협회 간 데이터 공유로 치료 효과를 제고하며 공중보건 영역에선 전국의 의료 데이터를 연계하여 전염병 발생과 같은 긴박한 순간에 빠른 의사결정을 가능케 할 전망이다.[38]

한편, 의료 분야에서 빅 데이터가 효과를 발휘하기 위해서는 대량의 의료정보 수집이 필수적이기 때문에, 개인정보의 보호와 빅 데이터 활용이라는 두 가지 가치가 상충하게 된다. 따라서, 의료 분야에서 빅 데이터의 활용과 보급을 위해서는 이러한 문제에 대한 가이드라인 마련이 필요한 상태이다.[39]

기업 경영
대규모의 다양한 데이터를 활용한 '빅데이터 경영'이 주목받으면서 데이터 품질을 높이고 방대한 데이터의 처리를 돕는 데이터 통합(Data Integration)의 중요성이 부각되고 있다.

데이터 통합(DI)은 데이터의 추출, 변환, 적재를 위한 ETL 솔루션이 핵심인데 ETL 솔루션을 활용하면 일일이 수많은 데이터를 기업 데이터 포맷으로 코딩하지 않아도 되고 데이터 품질을 제고할 수 있기 때문에 DI는 빅데이터 환경에 꼭 필요한 데이터 솔루션으로 평가받고 있는 단계까지 진입되었다.

한편 비즈니스 인텔리전스(Business Intelligence, BI)보다 진일보한 빅데이터 분석 방법이 비즈니스 애널리틱스(Business analytics, BA)인데 고급분석 범주에 있는 BA는 기본적으로 BI를 포함하면서도 미래 예측 기능과 통계분석, 확률 분석 등을 포함해 최적의 데이터 기반 의사결정을 가능케 하는 것으로 평가받고 있기도 하다.[40]

마케팅
인터넷으로 시작해서 인터넷으로 마감하는 생활, 스마트폰을 이용해 정보를 검색하고 쇼핑도하고 SNS를 이용해서 실시간으로 글을 남기는 등의 다양하게 인터넷을 이용하는 동안 남는 흔적같은 모인 데이터들을 분석하면 개인의 생활 패턴, 소비성향 등을 예측할 수 있고 기업들은 이런 데이터를 통해서 소비자가 원하는 것들을 미리 예측할 수 있다. 빅 데이터가 마케팅 자료로 활용되는 사례이다.[40]

마케팅 분석의 예로써 지역축제의 SNS 분석을 들 수 있다. 기존 지역축제의 성공을 측정하는 방식은 경제적 파급효과를 통해 이루어져 왔다. 하지만 축제의 성공 지표는 개최에 따르는 경제성만으로는 평가될 수 없으며, 축제를 즐기는 관광객의 즐겁고 신나는 경험을 통해 투자와 소비로 연결되는 선순환 과정을 확보해야 한다. 이를 측정할 방법은 축제를 즐긴 관광객이 남긴 웹상의 '5Ns'의 추적을 통해 가능하다.[41] 먼저, 주목(attentioN, SNS 게시물 및 영상 조회수와 도달범위 등으로 측정) 정도와 2단계인 반응(reactioN, 좋아요, 공감, 비공감 등 표시 행위)을 통해 사람들의 축제에 대한 관심을 파악할 수 있다. 나아가 표현(expressioN, 게시물, 댓글 등 의사표현 행위)을 통해 축제에 대한 만족 및 불만족을 표현하게 되고, 온라인 공간을 넘어 오프라인 축제로의 참여(participatioN)와 소비(consumptioN)로 이어지게 된다. 5Ns 분석을 적용한 대구치맥페스티벌은 전국적 인지도와 폭넓은 대중성을 확보했음에도 마지막 단계인 소비(consumptioN)로의 연계 고리가 약한 것으로 나타났다. 일회성 행사의 한계를 극복하기 위해 축제의 상설화를 통해 축제 경험을 강화하는 물리적 공간의 확보가 제시되었다.[42]

기상정보
한반도 전역의 기상관측정보를 활용해 일기예보와 각종 기상특보 등 국가 기상서비스를 제공하고 있는 기상청은 정밀한 기상예측을 위한 분석 과정에서 발생하는 데이터 폭증에 대응하고자 빅데이터 저장시스템의 도입을 추진하였다.

대다수 스토리지 기업들의 솔루션을 검토한 끝에 한국 IBM의 고성능 대용량 파일공유시스템(General Parallel File System, 이하 GPFS)을 적용한 스토리지 시스템을 선택하였다고 밝혔다.

한국IBM이 기상청에 제공한 GPFS 기반의 빅데이터 저장시스템은 IBM 시스템 스토리지 제품군, 시스템 x서버 제품군과 고속 네트워킹 랙스위치(RackSwitch) 등이 통합돼 있는 시스템이다.[40]

보안관리
보안관리는 빅데이터 환경을 이용해 성장과 기술 발전을 동시에 이루는 분야로 분리한다. 클라우드 및 모바일 환경으로 접어들면서 물리/가상화 IT 시스템의 복잡성이 더욱 높아지고 있어 유무선 네트워크, 프라이빗/퍼블릭 클라우드, 모바일 애플리케이션과 기기관리 등 IT 시스템 전반에서 대대적인 변화가 예상되고 있어 막대한 양의 보안관리가 중요한 요소로 현실화되고 있다.[43]

구글 번역
구글에서 제공하는 자동 번역 서비스인 구글 번역은 빅 데이터를 활용한다. 지난 40년 간 컴퓨터 회사 IBM의 자동 번역 프로그램 개발은 컴퓨터가 명사, 형용사, 동사 등 단어와 어문의 문법적 구조를 인식하여 번역하는 방식으로 이뤄졌다. 이와 달리 2006년 구글은 수억 건의 문장과 번역문을 데이터베이스화하여 번역시 유사한 문장과 어구를 기존에 축적된 데이터를 바탕으로 추론해 나가는 통계적 기법을 개발하였다. 캐나다 의회의 수백만 건의 문서를 활용하여 영어-불어 자동번역 시스템개발을 시도한 IBM의 자동 번역 프로그램은 실패한 반면 구글은 수억 건의 자료를 활용하여 전 세계 58개 언어 간의 자동번역 프로그램 개발에 성공하였다. 이러한 사례로 미루어 볼 때, 데이터 양의 측면에서의 엄청난 차이가 두 기업의 자동 번역 프로그램의 번역의 질과 정확도를 결정했으며, 나아가 프로젝트의 성패를 좌우했다고 볼 수 있다.[40]

논쟁점
이 문단은 비어 있습니다.
내용을 추가해 주세요.
폐해 사례
케임브리지 애널리티카의 불법 데이터 사용
2019년 7월에 넷플릭스에서 개봉된 오리지널 다큐멘터리 <거대한 해킹(The Great Hack)>에서 파슨스 디자인 스쿨의 부교수 데이비드 캐롤(David Carroll)은 이렇게 말한다.

“	우리의 온라인 활동에서 나오는 데이터가 그냥 사라지진 않는다. 우리의 디지털 흔적들을 모으고 분석하면 매년 1조 달러 규모의 산업이 된다. 우린 이제 원자재가 된 것이다. 그럼에도 불구하고, 누구도 이용 조건을 읽어보려고 하지 않는다. 우리의 모든 교류 내역과 신용카드 결제, 웹 검색, 위치 정보, ‘좋아요’까지 우리의 신원과 결부되어 실시간으로 수집된다. 그 데이터를 구매하는 누구든, 우리의 감정의 고동에 곧바로 접속할 수 있다. 그들은
이런 지식으로 무장하고 우리의 관심을 끌기 위해 경쟁한다. 개인 맞춤형으로 각자 혼자만 보는 콘텐츠를 지속적으로 제공하면서. 이것은 우리 모두에게 해당되는 진실이다.
-《거대한 해킹》 내용 中[44]

”
데이비드 캐롤은 2016년 미국 대통령 선거와 2016년 브렉시트 국민투표에 케임브리지 애널리티카(Cambridge Analytica)가 깊이 관여해 있음을 밝히려고 애쓰면서 영국의 법을 이용해서 캐임브리지 애널리티카가 보유하고 있다고 여겨지는 데이터를 되찾아오려고 노력하고 있다. 그는 런던 소재 고등 법원에 케임브리지 애널리티카와 SCL 선거 캠페인회사(SCL Elections Ltd)를 언급하며 자신의 데이터를 복구하고 그 출처를 공개하라는 성명을 제출했다. 영국 보수당 국회의원 다미안 콜린스(Damian Noel Thomas Collins MP)가 케임브리지 애널리티카의 대표인 알렉산더 닉스(Alexander Nix)를 법정에 불러서 심문을 받게 했고, 페이스북의 대표이사 마크 주커버그와 케임브리지 애널리티카의 내부고발자 크리스토퍼 와일리(Christopher Wylie)를 참고인으로 불러 조사가 시작되었다. 데이비드 캐롤은 빅데이터 해킹의 위험에 대해 경고하면서, 대서양 양측에서 규제 압력을 가해서 전세계 기업들이 개인 정보 취급에 대해 보다 투명하게 만들게 해야 한다고 주장을 계속하고 있다.[45]